---
author: "Fernando Alvarado"
date: "2025-03-12"
output: html_document
---


```{r setup, include=FALSE}
#Empezamos limpiando nuestro ambiente
rm(list = ls(all.names = TRUE))


# Configuración global de los bloques de código (chunk's)
knitr::opts_chunk$set(
	echo = FALSE,
	fig.align = "center",
	fig.dim = c(5.0, 4.0),
	fig.pos = "H",
#Agregamos configuraciones para evitar mensajes de advertencias y de errores en el archivo
	message = FALSE,
	warning = FALSE,
	error = F
)



# Librerias
library(dplyr)      # Para el manejo de datos
library(tidyr)
library(tibble)    # Manejo de df
library(ggplot2)    # Para realizar gráficas
library(kableExtra) # Para un mejor manejo de tablas
library(knitr)
library(purrr)      # Para la función map y map2
# Para purebas de hipotesis
library(multcomp)   # Para pruebas de hipótesis
library(car)        # Para funciones útiles de modelos de regresión lineal múltiple
library(broom)      # Para obtener los residuales estandarizados
library(purrr)      # Para la función map y map2
library(lmtest )    #Checar homoceasticidad
library(nortest )

library(esquisse) # Graficacion sencilla

# Libreria datos
library(mlbench)      # Para cargar el dataset de mlbench)

# Libreria eleccion modelos 
library(glmnet)
library(stats )
library(bestglm) # Paquete para buscar el mejor subset de un glm 
library(scales)
```



```{r Reultados_Datos}

# Cargar el dataset
data("PimaIndiansDiabetes")

# Ver las primeras filas
head(PimaIndiansDiabetes)

df_resultados <- data.frame(
  Metodo = character(),
  AIC = numeric(),
  numero_bettas = integer(),
  variables_select = character(), 
  
  stringsAsFactors = FALSE
)


nueva_fila_resultados <- function( metodo, AIC, numero_bettas, vars) {
  fila <- data.frame(
    Metodo = metodo,
    AIC = AIC,
    numero_bettas = numero_bettas,
    variables_select = vars,
    stringsAsFactors = FALSE
  )
  
  df_resultados <- rbind(df_resultados, fila)
  return(df_resultados)
  
}



```



```{r}
ln_diabetes <- data.frame(
      pregnant = log(PimaIndiansDiabetes$pregnant +1),
      glucose = log(PimaIndiansDiabetes$glucose +1),
      pressure = log(PimaIndiansDiabetes$pressure +1),
      triceps = log(PimaIndiansDiabetes$triceps +1),
      insulin = log(PimaIndiansDiabetes$insulin +1),
      mass = log(PimaIndiansDiabetes$mass +1),
      pedigree = log(PimaIndiansDiabetes$pedigree +1),
      age = log(PimaIndiansDiabetes$age +1),
      diabetes = PimaIndiansDiabetes$diabetes
)


```



```{r Modelos}
#Modelo logit
modelo <- glm(diabetes ~ ., family = binomial(link = "logit"), data = PimaIndiansDiabetes)
modelo_nulo <- glm(diabetes ~ 1, family = binomial(link = "logit"), data = PimaIndiansDiabetes)
modelo2 <- " (.)^2 + I(pregnant^2)+ I(glucose^2) +I(pressure^2)+I(triceps^2)+I(insulin^2)+I(mass^2)+I(pedigree^2)+I(age^2)"
# Ver resumen

```
```{r}
library(tibble)
df_coeficientes <- modelo$coefficients %>%
                   enframe(name = "predictor", value = "coeficiente")

df_coeficientes %>%
 filter(predictor != "(Intercept)") %>%
 ggplot() +
  aes(x = predictor, y = coeficiente) +
  geom_col(fill = "#0C4C8A") +
  theme_minimal()

```

Grafica donde podemos observar los coefieicnetes de nuestro modelo, agregando todas las variables.


```{r Mejor_subset}

mejor_Subset <- function(link = "logit", data = PimaIndiansDiabetes){
    dataBest <- data %>%
    mutate(y = ifelse(as.character(diabetes) == "pos", 1, 0)) %>%
    dplyr::select( pregnant, glucose, pressure, triceps, insulin, mass, pedigree, age, y)

    dataBest <- na.omit(dataBest)

    modelo_best <- bestglm(
        Xy = dataBest,
        family = binomial(link = link),      
        IC = "AIC",
        method = "exhaustive"
    )
    return(modelo_best)
}

```







```{r Stepwise}

StepwiseBack <- function(modelo, link = "logit",  data = PimaIndiansDiabetes){
   formula <- as.formula(paste("diabetes ~", modelo ))
   model <- glm(formula, family = binomial(link = link), data = data)
   #Haciendolo Backward
   modelo_step <- step(
      object = model,
      direction = "backward",
      scope = list(upper = model, lower = ~1),
      trace = FALSE
  )
  return(modelo_step) 
}



# Ejemplos de las demas formas de hacer Stepwise

#Haciendolo Fordward
#modelo_forward <- step(
#  object = modelo_nulo,
#  direction = "forward",
#  scope = list(lower = ~1, upper =  modelo),
#  trace = FALSE
#)



# En ambas direcciones
#modelo_both <- step(
#  object = modelo_nulo,
#  scope = list(lower = ~1, upper = modelo),
#  direction = "both",
#  trace = FALSE
#)

```


```{r Funcion_lasso}
#Funcion para genralizar la funcion lasso y pdoer evalur distintos modelos
#Parametros:
# modelo: modelo de regresion a evaluar 

Rugalarizacion_Lasso <- function(modelo, link = "logit", data = PimaIndiansDiabetes){
    formula <- as.formula(paste("diabetes ~", modelo ))
    x <- model.matrix(formula , data = data)[, -1] #Quitando el intercepto 
    y <- data$diabetes  #Variables binarias de si el paciente tiene o no diabetes

    #Ajusta el modelo lasso a nuestro modelo de regresion  sirve para ver las trayectoras de nuestro parametro lambda
    modelo_lasso <- glmnet(
    x = x,
    y = y,
    family = binomial(link = link),  
    alpha = 1,          
    standardize = TRUE,
    nlambda = 100
  )

  #Ahora toca elegir el mejor lambda, para eso utilizaremos la validacion cruzada
  cv_error <- cv.glmnet(
              x      = x,
              y      = y,
              alpha  = 1,
              nfolds = 10,
              family = binomial(link = link),  
              type.measure = "deviance",
              standardize  = TRUE
           )
  
   modelOut <- glmnet(
            x           = x,
            y           = y,
            family = binomial(link = link),  
            alpha       = 1,
            lambda      = cv_error$lambda.1se,
            standardize = TRUE
    )
  
  df_coeficientes <- coef(modelOut) %>%
                   as.matrix() %>%
                   as_tibble(rownames = "predictor") %>%
                   rename(coeficiente = s0)
                   
  out <- df_coeficientes %>%
      filter(
        predictor != "(Intercept)",
        coeficiente != 0
      ) 
  salida <- list(
      modelo_lasso = modelo_lasso,
      cv_error = cv_error, 
      coefs = out
  )
  return(salida)
}

#Fucnion para graficar como se ve la caide del parametro lambda en el ajuste lasso 
#Parametros:
# modelo: model, tenemos que pasar la salida de model_lasso, para que funcione y se calcule de la funcion Regularizacion_lasso

graficacion_lasso <- function(param, data = PimaIndiansDiabetes){
  model_lasso = param$modelo_lasso
  regularizacion <- model_lasso$beta %>% 
                  as.matrix() %>%
                  t() %>% 
                  as_tibble() %>%
                  mutate(lambda = model_lasso$lambda)

regularizacion <- regularizacion %>%
                   pivot_longer(
                     cols = !lambda, 
                     names_to = "predictor",
                     values_to = "coeficientes"
                   )

regularizacion %>%
  ggplot(aes(x = lambda, y = coeficientes, color = predictor)) +
  geom_line() +
  scale_x_log10(
    breaks = trans_breaks("log10", function(x) 10^x),
    labels = trans_format("log10", math_format(10^.x))
  ) +
  labs(title = "Coeficientes del modelo en función de la regularización") +
  theme_bw() +
  theme(legend.position = "none")
}



#Formula para recrear el modelo, a partir de las variables que nos dio la seleccion lasso
#Parametros:
# params: Modelo de lasso que le metemos a nuestro algoritmo para que funciones 
# link: tipo de link que queremos usar, por defecto es logit

AIC_lasso <- function(params, link="logit", data = PimaIndiansDiabetes){
    coeficientes <-   params$coefs$predictor#
    formula_GLM <- as.formula(paste("diabetes ~", paste(coeficientes, collapse = "+"))) 
    model <- glm(formula_GLM, family = binomial(link = link), data = data)  
    sal <- list(
        model = model,
        AIC = AIC(model)
    )  
     return(sal)
}

# graficacion_lasso(simple )
```



```{r}
joinData <- function(lista){
  vars_sin_intercept <- setdiff(lista, "(Intercept)")
  string_vars <- paste(vars_sin_intercept, collapse = ",")
  return(string_vars)
}

```






! limpiar los comentarios 

```{r Procesar_resultados}

# Pregunta A
besSubset <- mejor_Subset(link = "logit")
df_resultados <-nueva_fila_resultados( "Mejor Subset logit", besSubset$BestModel$aic, length(besSubset$BestModel$coefficients)-1, names())


modeloInteracciones <- StepwiseBack(" . ", "logit") # Modelo simple Stepwise
df_resultados <- nueva_fila_resultados("Stepwise_Backward logit", AIC(modeloInteracciones), length(modeloInteracciones$coefficients)-1, names())


simple <- Rugalarizacion_Lasso(" . ", "logit")   #Ejecutando nuestro primer modelo lasso
df_resultados <- nueva_fila_resultados("Lasso logit", AIC_lasso(simple, "logit")$AIC, length(simple$coefs$predictor), names())


# Pregunta B

StepB <- StepwiseBack(modelo2, link = "logit") # Modelo cuadratico Stepwise
df_resultados <- nueva_fila_resultados("Stepwise_Backward_Cuadratico logit", AIC(StepB), length(StepB$coefficients)-1, names())


lassoB <- Rugalarizacion_Lasso(modelo2, link = "logit")   #Ejecutando nuestro primer modelo lasso
df_resultados <- nueva_fila_resultados("Lasso_Cuadratico logit", AIC_lasso(lassoB, "logit")$AIC, length(lassoB$coefs$predictor), names())

# Pregunta C

#Usando liga probit

# Pregunta  ----A----
besSubsetProbit <- mejor_Subset(link = "probit")
df_resultados <-nueva_fila_resultados( "Mejor Subset probit", besSubsetProbit$BestModel$aic, length(besSubsetProbit$BestModel$coefficients)-1, names(besSubsetProbit$BestModel$coefficients))


modeloInteraccionesProbit <- StepwiseBack(" . ", "probit") # Modelo simple Stepwise
df_resultados <- nueva_fila_resultados("Stepwise_Backward probit", AIC(modeloInteraccionesProbit), length(modeloInteraccionesProbit$coefficients)-1, names(modeloInteraccionesProbit$coefficients))


simpleProbit <- Rugalarizacion_Lasso(" . ", "probit")   #Ejecutando nuestro primer modelo lasso
df_resultados <- nueva_fila_resultados("Lasso probit", AIC_lasso(simpleProbit, "probit")$AIC, length(simpleProbit$coefs$predictor)-1, names(simpleProbit$coefs$predictor))


# Pregunta  ----B----

StepBprobit <- StepwiseBack(modelo2, link = "probit") # Modelo cuadratico Stepwise
df_resultados <- nueva_fila_resultados("Stepwise_Backward_Cuadratico probit", AIC(StepBprobit), length(StepBprobit$coefficients)-1, names(StepBprobit$coefficients))


lassoBprobit <- Rugalarizacion_Lasso(modelo2, link = "probit")   #Ejecutando nuestro primer modelo lasso
df_resultados <- nueva_fila_resultados("Lasso_Cuadratico probit", AIC_lasso(lassoBprobit, "probit")$AIC, length(lassoBprobit$coefs$predictor)-1, names(lassoBprobit$coefs$predictor))



#Usando liga cloglog

# Pregunta  ----A----
besSubsetcloglog <- mejor_Subset(link = "cloglog")
df_resultados <-nueva_fila_resultados( "Mejor Subset cloglog", besSubsetcloglog$BestModel$aic, length(besSubsetcloglog$BestModel$coefficients)-1, names(besSubsetcloglog$BestModel$coefficients))


modeloInteraccionescloglog <- StepwiseBack(" . ", "cloglog") # Modelo simple Stepwise
df_resultados <- nueva_fila_resultados("Stepwise_Backward cloglog", AIC(modeloInteraccionescloglog), length(modeloInteraccionescloglog$coefficients)-1, names(modeloInteraccionescloglog$coefficients))


simplecloglog <- Rugalarizacion_Lasso(" . ", "cloglog")   #Ejecutando nuestro primer modelo lasso
df_resultados <- nueva_fila_resultados("Lasso cloglog", AIC_lasso(simplecloglog, "cloglog")$AIC, length(simplecloglog$coefs$predictor)-1, names(simplecloglog$coefs$predictor))


# Pregunta  ----B----

StepBcloglog <- StepwiseBack(modelo2, link = "cloglog") # Modelo cuadratico Stepwise
df_resultados <- nueva_fila_resultados("Stepwise_Backward_Cuadratico cloglog", AIC(StepBcloglog), length(StepBcloglog$coefficients)-1, names(StepBcloglog$coefficients))


lassoBcloglog <- Rugalarizacion_Lasso(modelo2, link = "cloglog")   #Ejecutando nuestro primer modelo lasso
df_resultados <- nueva_fila_resultados("Lasso_Cuadratico cloglog", AIC_lasso(lassoBcloglog, "cloglog")$AIC, length(lassoBcloglog$coefs$predictor)-1, names(lassoBcloglog$coefs$predictor))

# cambio de datos

```



```{r}
# Pregunta A
besSubset <- mejor_Subset(link = "logit", ln_diabetes)
df_resultados <-nueva_fila_resultados( "Mejor Subset logit", besSubset$BestModel$aic, length(besSubset$BestModel$coefficients)-1, joinData(names(besSubset$BestModel$coefficients)))


modeloInteracciones <- StepwiseBack(" . ", "logit", ln_diabetes) # Modelo simple Stepwise
df_resultados <- nueva_fila_resultados("Stepwise_Backward logit", AIC(modeloInteracciones), length(modeloInteracciones$coefficients)-1, joinData(names(modeloInteracciones$coefficients)))


simple <- Rugalarizacion_Lasso(" . ", "logit", ln_diabetes)   #Ejecutando nuestro primer modelo lasso


df_resultados <- nueva_fila_resultados("Lasso logit", AIC_lasso(simple, "logit")$AIC, length(simple$coefs$predictor), joinData(simple$coefs$predictor))


# Pregunta B

StepB <- StepwiseBack(modelo2, link = "logit", ln_diabetes) # Modelo cuadratico Stepwise
df_resultados <- nueva_fila_resultados("Stepwise_Backward_Cuadratico logit", AIC(StepB), length(StepB$coefficients)-1, joinData(names(StepB$coefficients)))


lassoB <- Rugalarizacion_Lasso(modelo2, link = "logit", ln_diabetes)   #Ejecutando nuestro primer modelo lasso
df_resultados <- nueva_fila_resultados("Lasso_Cuadratico logit", AIC_lasso(lassoB, "logit")$AIC, length(lassoB$coefs$predictor)  , joinData(lassoB$coefs$predictor))
```

```{r}
length(df_resultados$variables_select)


```


```{r}
names(besSubset$BestModel$coefficients)



b <- joinData(names(StepB$coefficients))
b
```




```{r}
a = c()
a <-   names(besSubset$BestModel$coefficients)

a


typeof(a)
```






Referencias (Inspiracion de graficas):

Selección de predictores: subset selection, ridge, lasso y reducción de dimensionalidad por Joaquín Amat Rodrigo, disponible con licencia CC BY-NC-SA 4.0 en https://www.cienciadedatos.net/documentos/31_seleccion_de_predictores_subset_selection_ridge_lasso_dimension_reduction

































































