---
author: "Fernando Alvarado"
date: "2025-03-12"
output: html_document
---


```{r setup, include=FALSE}
#Empezamos limpiando nuestro ambiente
rm(list = ls(all.names = TRUE))


# Configuración global de los bloques de código (chunk's)
knitr::opts_chunk$set(
	echo = FALSE,
	fig.align = "center",
	fig.dim = c(5.0, 4.0),
	fig.pos = "H",
#Agregamos configuraciones para evitar mensajes de advertencias y de errores en el archivo
	message = FALSE,
	warning = FALSE,
	error = F
)



# Librerias
library(dplyr)      # Para el manejo de datos
library(tidyr)
library(tibble)    # Manejo de df
library(ggplot2)    # Para realizar gráficas
library(kableExtra) # Para un mejor manejo de tablas
library(knitr)
library(purrr)      # Para la función map y map2
# Para purebas de hipotesis
library(multcomp)   # Para pruebas de hipótesis
library(car)        # Para funciones útiles de modelos de regresión lineal múltiple
library(broom)      # Para obtener los residuales estandarizados
library(purrr)      # Para la función map y map2
library(lmtest )    #Checar homoceasticidad
library(nortest )

library(esquisse) # Graficacion sencilla

# Libreria datos
library(mlbench)      # Para cargar el dataset de mlbench)

# Libreria eleccion modelos 
library(glmnet)
library(stats )
library(bestglm) # Paquete para buscar el mejor subset de un glm 
library(scales)
```



```{r Reultados_Datos}

# Cargar el dataset
data("PimaIndiansDiabetes")

# Ver las primeras filas
head(PimaIndiansDiabetes)

df_resultados <- data.frame(
  Metodo = character(),
  AIC = numeric(),
  numero_bettas = integer(),
  
  stringsAsFactors = FALSE
)





nueva_fila_resultados <- function( metodo, AIC, numero_bettas) {
  fila <- data.frame(
    Metodo = metodo,
    AIC = AIC,
    numero_bettas = numero_bettas,
    stringsAsFactors = FALSE
  )
  
  df_resultados <- rbind(df_resultados, fila)
  return(df_resultados)
  
}



```


```{r Modelo_logit}
#Modelo logit
modelo <- glm(diabetes ~ ., family = binomial(link = "logit"), data = PimaIndiansDiabetes)
modelo_nulo <- glm(diabetes ~ 1, family = binomial(link = "logit"), data = PimaIndiansDiabetes)
# Ver resumen


summary(modelo)



```
```{r}
library(tibble)
df_coeficientes <- modelo$coefficients %>%
                   enframe(name = "predictor", value = "coeficiente")

df_coeficientes %>%
 filter(predictor != "(Intercept)") %>%
 ggplot() +
  aes(x = predictor, y = coeficiente) +
  geom_col(fill = "#0C4C8A") +
  theme_minimal()

```

Grafica donde podemos observar los coefieicnetes de nuestro modelo, agregando todas las variables.


```{r Mejor_subset}

dataBest <- PimaIndiansDiabetes %>%
  mutate(y = ifelse(as.character(diabetes) == "pos", 1, 0)) %>%
  dplyr::select( pregnant, glucose, pressure, triceps, insulin, mass, pedigree, age, y)

dataBest <- na.omit(dataBest)

modelo_best <- bestglm(
  Xy = dataBest,
  family = binomial,      
  IC = "AIC",
  method = "exhaustive"
)



df_resultados <-nueva_fila_resultados( "Mejor Subset", modelo_best$BestModel$aic, length(modelo_best$BestModel$coefficients)-1)





```





```{r Stepwise}

StepwiseBack <- function(modelo, link = "logit"){
   formula <- as.formula(paste("diabetes ~", modelo ))
   model <- glm(formula, family = binomial(link = link), data = PimaIndiansDiabetes)
   #Haciendolo Backward
   modelo_step <- step(
      object = model,
      direction = "backward",
      scope = list(upper = model, lower = ~1),
      trace = FALSE
  )
  return(modelo_step) 
}


modeloInteracciones <- StepwiseBack(" . ")
         # Ver el AIC del modelo final

df_resultados <- nueva_fila_resultados("Stepwise_Backward", AIC(modeloInteracciones), length(modeloInteracciones$coefficients)-1)
#Haciendolo Fordward
#modelo_forward <- step(
#  object = modelo_nulo,
#  direction = "forward",
#  scope = list(lower = ~1, upper =  modelo),
#  trace = FALSE
#)



# En ambas direcciones
#modelo_both <- step(
#  object = modelo_nulo,
#  scope = list(lower = ~1, upper = modelo),
#  direction = "both",
#  trace = FALSE
#)

```


```{r Funcion_lasso}
#Funcion para genralizar la funcion lasso y pdoer evalur distintos modelos
#Parametros:
# modelo: modelo de regresion a evaluar 

Rugalarizacion_Lasso <- function(modelo, link = "logit"){
    formula <- as.formula(paste("diabetes ~", modelo ))
    x <- model.matrix(formula , data = PimaIndiansDiabetes)[, -1] #Quitando el intercepto 
    y <- PimaIndiansDiabetes$diabetes  #Variables binarias de si el paciente tiene o no diabetes

    #Ajusta el modelo lasso a nuestro modelo de regresion  sirve para ver las trayectoras de nuestro parametro lambda
    modelo_lasso <- glmnet(
    x = x,
    y = y,
    family = binomial(link = link),  
    alpha = 1,          
    standardize = TRUE,
    nlambda = 100
  )

  #Ahora toca elegir el mejor lambda, para eso utilizaremos la validacion cruzada
  cv_error <- cv.glmnet(
              x      = x,
              y      = y,
              alpha  = 1,
              nfolds = 10,
              family = binomial(link = link),  
              type.measure = "deviance",
              standardize  = TRUE
           )
  
  modelOut <- glmnet(
            x           = x,
            y           = y,
            family = binomial(link = link),  
            alpha       = 1,
            lambda      = cv_error$lambda.1se,
            standardize = TRUE
  )
  
 df_coeficientes <- coef(modelOut) %>%
                   as.matrix() %>%
                   as_tibble(rownames = "predictor") %>%
                   rename(coeficiente = s0)
                   
 out <- df_coeficientes %>%
      filter(
        predictor != "(Intercept)",
        coeficiente != 0
      ) 
  
 
    
  
  
  
  salida <- list(
    modelo_lasso = modelo_lasso,
    cv_error = cv_error, 
    coefs = out
  )
  return(salida)
}

#Fucnion para graficar como se ve la caide del parametro lambda en el ajuste lasso 
#Parametros:
# modelo: model, tenemos que pasar la salida de model_lasso, para que funcione y se calcule de la funcion Regularizacion_lasso

graficacion_lasso <- function(param){
  model_lasso = param$modelo_lasso
  regularizacion <- model_lasso$beta %>% 
                  as.matrix() %>%
                  t() %>% 
                  as_tibble() %>%
                  mutate(lambda = model_lasso$lambda)

regularizacion <- regularizacion %>%
                   pivot_longer(
                     cols = !lambda, 
                     names_to = "predictor",
                     values_to = "coeficientes"
                   )

regularizacion %>%
  ggplot(aes(x = lambda, y = coeficientes, color = predictor)) +
  geom_line() +
  scale_x_log10(
    breaks = trans_breaks("log10", function(x) 10^x),
    labels = trans_format("log10", math_format(10^.x))
  ) +
  labs(title = "Coeficientes del modelo en función de la regularización") +
  theme_bw() +
  theme(legend.position = "none")
}


AIC_lasso <- function(modelo, cv_error){
  #Preparacion data
  formula <- as.formula(paste("diabetes ~", modelo ))
  x <- model.matrix(formula , data = PimaIndiansDiabetes)[, -1] #Quitando el intercepto 
  y <- ifelse(PimaIndiansDiabetes$diabetes == "pos", 1, 0)

  pred_prob <- predict(cv_error, newx = x, s = "lambda.min", type = "response") #Predicciones del modelo, con la lambda minima

  logLik_lasso <- sum(y * log(pred_prob) + (1 - y) * log(1 - pred_prob)) # Calcular log-verosimilitud del modelo

  # Número de coeficientes distintos de cero (excluyendo intercepto)
  coef_lasso <- coef(cv_error, s = "lambda.min")
  k <- sum(coef_lasso[-1] != 0)

  # Calcular AIC
  AIC_lasso <- -2 * logLik_lasso + 2 * k
  AIC_lasso
  
  #Regreso de nuestro df
  datosLasso <- list(
      coef = coef(cv_error, s = "lambda.min"), 
      AIC = AIC_lasso
  )
  
  return(datosLasso)
}


```



```{r}
simple <- Rugalarizacion_Lasso(" . ", "probit")
graficacion_lasso(simple )



print(simple$coefs)

print(length(simple$coefs$predictor)) 

#print(AIC_lasso(" . ", simple$cv_error))

df_resultados <- nueva_fila_resultados("Lasso", 0, length(simple$coefs$predictor))

```

 






```{r}

head(PimaIndiansDiabetes)
```





-------------------------------------------------------------------------------------------------------------------------------------



```{r}
modeloInteracciones <- glm(diabetes ~ (.)^2 + I(pregnant^2)+ I(glucose^2) +I(pressure^2)+I(triceps^2)+I(insulin^2)+I(mass^2)+I(pedigree^2)+I(age^2) , family = binomial(link = "logit"), data = PimaIndiansDiabetes)

```














Referencias (Inspiracion de graficas):

Selección de predictores: subset selection, ridge, lasso y reducción de dimensionalidad por Joaquín Amat Rodrigo, disponible con licencia CC BY-NC-SA 4.0 en https://www.cienciadedatos.net/documentos/31_seleccion_de_predictores_subset_selection_ridge_lasso_dimension_reduction










































